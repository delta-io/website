---
description: Learn how Delta table protocols are versioned.
orphan: 1
---

import { ApacheSpark, DeltaLake } from "/src/components/Attributions";
import { CBCreateTable } from "/src/components/CodeBlocks";

<a id="table-version"></a>

# Table protocol versioning

The transaction log for a Delta table contains protocol versioning information
that supports <DeltaLake /> evolution. <DeltaLake /> tracks minimum [reader and writer
versions](/delta/delta-utility.md#detail-schema) separately.

<DeltaLake /> guarantees *backward compatibility*. A higher protocol version of
the
<DeltaLake /> reader is always able to read data that was written by a lower protocol
version.

<DeltaLake /> will occasionally break *forward compatibility*. Lower protocol
versions of <DeltaLake /> may not be able to read and write data that was
written by a higher protocol version of <DeltaLake />. If you try to read and
write to a table with a protocol version of <DeltaLake /> that is too low,
you'll get an error telling you that you need to upgrade.

When creating a table, <DeltaLake /> chooses the minimum required protocol version
based on table characteristics such as the schema or table properties. You can
also set the default protocol versions by setting the SQL configurations:

- `spark.databricks.delta.properties.defaults.minWriterVersion = 2` (default)
- `spark.databricks.delta.properties.defaults.minReaderVersion = 1` (default)

To upgrade a table to a newer protocol version, use the
`DeltaTable.upgradeTableProtocol` method:

<CBCreateTable
      sql={
    <div>

```sql
-- Upgrades the reader protocol version to 1 and the writer protocol version to 3.
ALTER TABLE <table_identifier> SET TBLPROPERTIES('delta.minReaderVersion' = '1', 'delta.minWriterVersion' = '3')
```

</div>
      }
    />

<CBCreateTable
      python={
    <div>

```python
from delta.tables import DeltaTable
delta = DeltaTable.forPath(spark, "path_to_table") # or DeltaTable.forName
delta.upgradeTableProtocol(1, 3) # upgrades to readerVersion=1, writerVersion=3
```

</div>
      }
    />

<CBCreateTable
      scala={
    <div>

```scala
import io.delta.tables.DeltaTable
val delta = DeltaTable.forPath(spark, "path_to_table") // or DeltaTable.forName
delta.upgradeTableProtocol(1, 3) // Upgrades to readerVersion=1, writerVersion=3
```

</div>
      }
    />

## Features by protocol version

| Feature             | `minWriterVersion` | `minReaderVersion` | Introduced in                                                             | Documentation                                        |
| ------------------- | ------------------ | ------------------ | ------------------------------------------------------------------------- | ---------------------------------------------------- |
| Basic functionality | 2                  | 1                  | --                                                                        | [Delta Lake guide](/index.md)                        |
| `CHECK` constraints | 3                  | 1                  | [Delta Lake 0.8.0](https://github.com/delta-io/delta/releases/tag/v0.8.0) | [\_](/delta-constraints.md#check-constraint)         |
| Generated columns   | 4                  | 1                  | [Delta Lake 1.0.0](https://github.com/delta-io/delta/releases/tag/v1.0.0) | [\_](/delta-batch.md#use-generated-columns)          |
| Column mapping      | 5                  | 2                  | [Delta Lake 1.2.0](https://github.com/delta-io/delta/releases/tag/v1.2.0) | [Column mapping](#column-mapping)                    |
| Change data feed    | 4                  | 1                  | [Delta Lake 2.0.0](https://github.com/delta-io/delta/releases/tag/v2.0.0) | [Change data feed](/delta/delta-change-data-feed.md) |

See [Requirements for
Readers](https://github.com/delta-io/delta/blob/master/PROTOCOL.md#requirements-for-readers)
and [Writer Version
Requirements](https://github.com/delta-io/delta/blob/master/PROTOCOL.md#writer-version-requirements)
in the [delta-io/delta](https://github.com/delta-io/delta) repo on the GitHub
website.

<a id="column-mapping"></a>

### Column mapping

[Column mapping
feature](https://github.com/delta-io/delta/blob/master/PROTOCOL.md#column-mapping)
allows Delta table columns and the underlying Parquet file columns to use
different names. This enables Delta schema evolution operations such as [RENAME
COLUMN](/delta/delta-batch.md#rename-columns) on a Delta table without the need
to rewrite the underlying Parquet files. It also allows users to name Delta
table columns by using [characters that are not
allowed](/delta/delta-batch.md#special-chars-in-col-name) by Parquet, such as
spaces, so that users can directly ingest CSV or JSON data into Delta without
the need to rename columns due to previous character constraints.

Column mapping requires upgrading the Delta Lake table protocol.

<Info title="Important" level="warning">
  Adding a constraint automatically upgrades the table writer protocol version.
  See{" "}
  <a href="https://delta.io/docs/spark/versioning">Table protocol versions</a>{" "}
  to understand table protocol versioning and what it means to upgrade the
  protocol version.
</Info>

<CBCreateTable
      sql={
    <div>

```sql
ALTER TABLE <table_name> SET TBLPROPERTIES (
   'delta.minReaderVersion' = '2',
   'delta.minWriterVersion' = '5',
   'delta.columnMapping.mode' = 'name'
)
```

</div>
      }
    />

<Info title="Note" level="info">
  The Delta table protocol specifies two modes of column mapping, by `name` and
  by `id`. Currently in Delta Lake only the `name` mode is supported.
</Info>
