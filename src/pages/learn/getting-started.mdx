---
title: 10 Minute Dip into Delta Lake
menu: learn
---

import * as React from "react";
import { TestTab, Examples } from "src/components/WipTabs";
import { Tab, Tabs, TabList, TabPanel } from "react-tabs";
import "react-tabs/style/react-tabs.css";

<TestTab
    pipPS={
<div>

### Set up interactive shell

To use Delta Lake interactively within the Spark Python shell, you need a local installation of Apache Spark. For all the instructions below make sure you install the correct version of Spark or PySpark that is compatible with Delta Lake 1.0.0. See the release compatibility matrix for details.

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   ```sh
   $ pip install pyspark==<compatible-spark-version>
   ```

2. Run PySpark with the Delta Lake package and additional configurations:

   ```sh
   $ pyspark --packages io.delta:delta-core_2.12:1.0.0 \
    --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
    --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
   ```

</div>
    }
    pipPP={
<div>

### Create and Configure a PySpark Session

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   ```sh
   $ pip install pyspark==<compatible-spark-version>
   ```

2. Create and configure a PySpark session:

   ```python
   import pyspark

   scala_version = '<compatible-scala-version>'
   delta_version = '<compatible-delta-version>'

   builder = (
       pyspark.sql.SparkSession.builder
         .appName('quickstart')
         .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')
         .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')
         .config('spark.jars.packages', f'io.delta:delta-core_{scala_version}:{delta_version}')
   )

   spark = builder.getOrCreate()
   ```

### Create a Table

- Create a DataFrame to write out in delta format.

  {" "}
  <Info title="Note" level="info">
    You can use existing Spark SQL code by changing the format from parquet,
    csv, json, and so on, to delta. Pandas DataFrames can also be used through
    PySpark's Pandas API.
  </Info>

    <Examples
      pyspark={
  <div>

  ```python
  df = spark.range(0, 5)
  df.show()
  ```

    </div>
      }
      pandas={
  <div>

  ```python
  import pyspark.pandas as ps
  import pandas as pd

  pdf = pd.DataFrame({'id': range(0, 5)})
  df = ps.from_pandas(pdf)
  df.show()
  ```

    </div>
      }
  />

  ```sh
  # output
  +---+
  | id|
  +---+
  |  0|
  |  1|
  |  2|
  |  3|
  |  4|
  +---+
  ```

- Write out your DataFrame as a Delta Table,

  ```python
  df.write.format('delta').save('/tmp/delta-table')
  ```

- Read from your Delta Table,

  ```python
  df = spark.read.format('delta').load('/tmp/delta-table')
  df.show()
  ```

  ```sh
  # output
  +---+
  | id|
  +---+
  |  0|
  |  1|
  |  2|
  |  3|
  |  4|
  +---+
  ```

</div>
    }
    pipDS={
<div>

### Set up interactive shell

To use Delta Lake interactively within the Spark Python shell, you need a local installation of Apache Spark. For all the instructions below make sure you install the correct version of Spark or PySpark that is compatible with Delta Lake 1.0.0. See the release compatibility matrix for details.

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   note: the optional [`delta-spark`](https://docs.delta.io/latest/api/python/index.html) Python package provides an enhanced python API for using delta lake.

   ```sh
   $ pip install pyspark==<compatible-spark-version> delta-spark
   ```

2. Run PySpark with the Delta Lake package and additional configurations:

   ```sh
   $ pyspark --packages io.delta:delta-core_2.12:1.0.0 \
     --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
     --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
   ```

</div>
    }
    pipDP={
<div>

### Create and Configure a PySpark Session

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   note: the optional [`delta-spark`](https://docs.delta.io/latest/api/python/index.html) Python package provides an enhanced python API for using delta lake.

   ```sh
   $ pip install pyspark==<compatible-spark-version> delta-spark
   ```

2. Create and configure a PySpark session:

   ```python
   import pyspark
   from delta import configure_spark_with_delta_pip

   builder = (
     pyspark.sql.SparkSession.builder
       .appName('quickstart')
       .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')
       .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')
   )

   spark = configure_spark_with_delta_pip(builder).getOrCreate()
   ```

</div>
    }
    condaPS={
<div>

### Set up interactive shell

To use Delta Lake interactively within the Spark Python shell, you need a local installation of Apache Spark. For all the instructions below make sure you install the correct version of Spark or PySpark that is compatible with Delta Lake 1.0.0. See the release compatibility matrix for details.

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   ```sh
   $ conda install pyspark==<compatible-spark-version>
   ```

2. Run PySpark with the Delta Lake package and additional configurations:

   ```sh
   $ pyspark --packages io.delta:delta-core_2.12:1.0.0 \
    --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
    --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
   ```

</div>
    }
    condaPP={
<div>

### Create and Configure a PySpark Session

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   ```sh
   $ conda install pyspark==<compatible-spark-version>
   ```

2. Create and configure a PySpark session:

   ```python
   import pyspark

   scala_version = '<compatible-scala-version>'
   delta_version = '<compatible-delta-version>'

   builder = (
       pyspark.sql.SparkSession.builder
         .appName('quickstart')
         .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')
         .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')
         .config('spark.jars.packages', f'io.delta:delta-core_{scala_version}:{delta_version}')
   )

   spark = builder.getOrCreate()
   ```

</div>
    }
    condaDS={
<div>

### Create and Configure a PySpark Session

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   note: the optional [`delta-spark`](https://docs.delta.io/latest/api/python/index.html) Python package provides an enhanced python API for using delta lake.

   ```sh
   $ conda install pyspark==<compatible-spark-version> delta-spark
   ```

2. Run PySpark with the Delta Lake package and additional configurations:

   ```sh
   $ pyspark --packages io.delta:delta-core_2.12:1.0.0 \
    --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
    --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
   ```

</div>
    }
    condaDP={
<div>

### Create and Configure a PySpark Session

1. Install the PySpark version that is compatible with the Delta Lake version by running the following:

   <Info title="Note" level="info">
     the optional{" "}
     <a href="https://docs.delta.io/latest/api/python/index.html">
       delta-spark
     </a>{" "}
     Python package provides an enhanced python API for using delta lake.
   </Info>

   ```sh
   $ conda install pyspark==<compatible-spark-version> delta-spark
   ```

2. Create and configure a PySpark session:

   ```python
   import pyspark

   scala_version = '<compatible-scala-version>'
   delta_version = '<compatible-delta-version>'

   builder = (
       pyspark.sql.SparkSession.builder
         .appName('quickstart')
         .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')
         .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')
         .config('spark.jars.packages', f'io.delta:delta-core_{scala_version}:{delta_version}')
   )

   spark = builder.getOrCreate()
   ```

</div>
    }
/>
