---
title: Data Skipping with Delta Lake
description: Learn how to use Delta Lake for advanced data skipping
thumbnail: ./thumbnail.png
author: Avril Aysha
date: 2025-11-21
---

This article explains what data skipping is and how you can use it to achieve faster query performance with Delta Lake.

You will learn how data skipping works and why data skipping performance varies significantly across different data storage formats. You will also learn about the different data skipping features in Delta Lake and how to choose the best one for your use case.

Let's jump in! ðŸª‚

## What is data skipping?

Data skipping is a strategy for query optimization that lets your query engine skip irrelevant rows in your dataset. It uses metadata (min/max, etc.) to avoid reading files or row groups that cannot match your predicate. Data skipping is a feature of your data storage format. 

Data skipping is most useful when running queries with selective filters. For example:

```python
df = (
    spark.read.format("delta").load("transactions")
    .select("id", "amount", "date")  # column pruning (skip other columns)
    .where("amount > 10000 AND date > '2024-01-01'")  # data skipping predicate
)
```

Delta Lake supports advanced data skipping. For the query above, Spark can easily figure out which rows and even entire files to skip when running this selective query. The â€œData Skipping with Delta Lakeâ€ section below goes into more detail about how this works with Delta Lake.

Data storage formats that support data skipping will allow query engines to skip reading data that is definitely not relevant to your query. In this example above, there are two optimization strategies happening at once:
1. all columns other than `amount` and `date` can be ignored (we call this column pruning)
2. all `amount` values below 10000 and `date` values before `2024-01-01` can be ignored (we call this data skipping).

Data skipping will save you lots of time and money on compute.

### Data Skipping Requirements
Data skipping depends on two core factors:

1. Metadata availability, and
2. Data layout

The data storage format needs to store metadata (such as column names and min/max values) in a way that query engines can easily access. Different metadata handling strategies will result in different data skipping performance. Data skipping can then be optimized even more through data layout strategies, e.g. physical partitioning, Z-ordering or liquid clustering. We will cover these data layout strategies towards the end of the article.

Let's take a look at the different data skipping capabilities of 3 common storage formats: CSV, Parquet and Delta Lake.

## Data skipping with CSV

CSV (Comma-Separated Values) is a common data storage format. It is a simple text-based format that stores data in a flat, sequential structure. CSV files do not give you much opportunity for data skipping.

When reading CSV files with a selective filter, query engines need to:

1. Read the file from beginning to end
2. Parse each line individually
3. Find rows that match your query

Query engines cannot optimize your query with data skipping much because CSV files are limited in the following ways:

- No built-in metadata about the data's content
- No predefined schema or data type information
- No way to skip irrelevant portions of the file

The only basic form of "data skipping" you might achieve with CSV files is at the file level. If your data is split across multiple CSV files (e.g. Hive-style partitioning) and you have a clear naming convention that indicates the content of each file, then you can manually instruct your query engine to only read specific CSV files. This is not true data skipping but more of a data organization strategy. It requires you to manually define which parts of the data to skip. Some query engines like Datafusion can automatically skip files based on filename patterns, but this is not the case for all engines.

If you're working with large datasets and/or often run selective queries, you'll probably want to avoid storing your data in CSV files.

## Data skipping with Parquet

[Apache Parquet](https://parquet.apache.org/) is a columnar data storage format with native metadata storage. Your data is stored in row groups and Parquet automatically collects statistics for each column in each row group, including:

- `min_value: minimum value in column`
- `max_value: maximum value in column`
- `null_count: count of null values in column`
- `num_rows: count of rows in row group`

This metadata gives you some nice opportunities for data skipping when running selective queries.

### Predicate Pushdown

Parquet's row-group statistics enable smart data skipping of irrelevant row groups. Your engine reads the statistics in the file footer and can skip row groups that can't possibly match the selective query. This is called â€œpredicate pushdownâ€.

For example:

```python
df = spark.read.format("parquet").load("transactions.parquet") \
.filter("amount > 10000")
```

For this query, your Parquet reader will:

1. Check row group statistics
2. Skip groups where `max(amount)` &lt;= 10000
3. Only read relevant row groups

This is much faster than reading all the rows in the dataset.

### Column Pruning

Since Parquet stores data in a columnar format, it allows your engine to skip reading entire columns that aren't needed for your query. This is called column pruning, and is really useful when you're reading specific columns from a wide dataset.

For example:

```python
summary_df = spark.read.parquet("transactions.parquet") \
    .select("transaction_date", "amount") \
    .filter("transaction_date > '2024-01-01'")
```

For this query, your engine will:

1. Only read `transaction_date` and `amount` columns
2. Skip all other columns in the Parquet file
3. Apply predicate pushdown on `transaction_date`

### Limitations of Parquet Data Skipping

Parquet gives you some good data skipping features like predicate pushdown and column pruning using row-group statistics. Some implementations also write extra metadata like Bloom filters. However, compared with Delta Lake's table-level metadata architecture, Parquet on its own has real limits:

- There is no table-level metadata index.
Parquet stores stats inside each file (footers/page indexes). Planners typically must touch many files (or at least their footers) to decide what to read. On object stores, that means lots of small I/Os and listing calls before any data scan.

- Skipping only happens within files.
Parquet's  skipping decides at the row-group level *within* each file. It doesn't provide a built-in way to keep a global, cross-file view of min/max to skip entire files up front. (Some engines may cache metadata, but that's engine-specific, not part of the format.)

- Advanced patterns are implementation-dependent.
Features like Bloom filters, page indexes, selective decoding, and row-level filtering exist, but aren't guaranteed across writers/readers. Behavior varies by engine and by how files were produced.

- No built-in layout management.
Parquet doesn't define how to reorganize data for better pruning across files. You can still partition directories or sort files, but coordinating and maintaining that layout is not standardizes and depends on external tools.

Delta Lake is built on top of Parquet to extend its data skipping features. Let's look at how this works.

## Data skipping with Delta Lake

Delta Lake is an [open table format] (https://delta.io/blog/open-table-formats/). This means that it stores your data _along with valuable metadata_ in an advanced storage framework that improves performance and reliability.

Delta Lake tables consist of:

1. Your tabular data stored using a best-in-class file format (Parquet)
2. Metadata stored in a separate transaction log

![](image1.png)

The Delta Lake transaction log unlocks great features like ACID transactions, time travel and more advanced data skipping. Read the [Delta Lake vs Data Lake](https://delta.io/blog/delta-lake-vs-data-lake/) article for more information on all of these features.

Let's look at how Delta Lake handles metadata and how this improves data skipping.

### Metadata Handling

Parquet files store column statistics for row groups in each file's footer. When you query a large dataset stored across multiple Parquet files, your engine needs to open each file to read these statistics. This becomes slow when you have lots of Parquet files, since each file needs to be accessed individually.

Delta Lake instead stores all file-level statistics in the centralized transaction log, which contains checkpoints and a record of all commits. This is smarter because it means query engines only have to fetch a small set of light-weight files (typically a checkpoint Parquet and a few JSON commits). This avoids opening lots of individual Parquet footers and usually makes planning much faster on object stores.

![](image2.png)

Because Delta Lake stores metadata at the file-level, query engines can skip entire files. This is more efficient than Parquet data skipping, which needs to fetch row-group metadata for each file.

### Different Data Skipping Techniques 
Delta Lake supports multiple data layout optimization techniques to further improve data skipping and make your queries run even faster. These techniques are:
1. Liquid Clustering
2. Z-ordering, and 
3. Hive-style physical partitioning

Note that in this article we are discussing Hive-style partitioning *with Delta Lake*. Traditional Hive-Style partitioning without Delta Lake requires engines to list directories to find partitions, whichrequires many I/O operations and is inefficient. Hive-style partitioning with Delta Lake still gives you the physical directory layout for compatibility, but the partition metadata is stored and accessed from the centralized Delta transaction log.

Let's start by looking at liquid clustering.

## Data skipping: Liquid Clustering
Liquid clustering is the most effective way to optimize your data layout. If you need faster queries and smarter storage, consider using liquid clustering for all your new Delta tables.

Especially when:

- Your data has skewed distributions.
- Filtering columns vary significantly in cardinality.
- You frequently ingest new data into tables.
- Your system handles multiple concurrent writes.
- Query patterns evolve over time.
- Traditional partitioning would create a small file problem.

Liquid clustering isn't compatible with Hive-style partitioning (with or without Delta Lake) or Z-ordering. Especially if your downstream systems rely on non-Delta Lake Hive-style partitioning, Liquid Clustering is likely not the right choice. 

Read more in the [Liquid Clustering tutorial](https://delta.io/blog/liquid-clustering/).

## Data skipping: Z-Ordering

[Z-Ordering](https://delta.io/blog/2023-06-03-delta-lake-z-order/) is a Delta Lake feature that stores related rows together within the same files. It sorts data across multiple dimensions using a clustering algorithm, so that rows with similar values are stored close to each other. This layout significantly speeds up queries on either or all of your clustering columns.

Z-Ordering gives you the opportunity to optimize your data for queries on multiple columns. This means you get more flexibility than with Hive-style partitioning. You can also combine Z-Ordering with physical partitioning in Delta Lake.

## Data skipping: Hive-style Partitioning

Traditional Hive-style partitioning *without Delta Lake* organizes partitions using the physical directory structure. For example:

```
spark-warehouse/sales_data
â”œâ”€â”€ region=East
â”‚   â”œâ”€â”€ part-00001.snappy.parquet
â”‚   â””â”€â”€ part-00002.snappy.parquet
â””â”€â”€ region=North
    â””â”€â”€ part-00003.snappy.parquet
â””â”€â”€ region=South
    â””â”€â”€ part-00004.snappy.parquet
â””â”€â”€ region=West
    â””â”€â”€ part-00005.snappy.parquet
```

To find files in a partition, query engines need to perform file listing operations.

These file listings can become a bottleneck when:

- Your data is stored in cloud object storage, which relies on key-value systems.
- You have many small partitions, leading to the Small File Problem.

Delta Lake avoids this issue. When performing Hive-style partitioning with Delta Lake, all metadata is stored in the transaction log instead of the file system. This allows Delta readers to directly access the transaction log to locate the required files for a query. Your query engine skips the expensive file listing process and runs your queries much faster.

## Data skipping with Delta Lake

This article has explained what data skipping is and how data skipping performance varies across common data storage formats: CSV, Parquet and Delta Lake.

Delta Lake offers great data skipping performance out of the box. You can further improve it by enabling liquid clustering. Delta Lake optimizes both your data layout and metadata handling so query engines can easily find the relevant data for your queries.
